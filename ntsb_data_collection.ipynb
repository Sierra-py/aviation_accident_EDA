{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a3c724",
   "metadata": {},
   "source": [
    "# Goal: Scrape the data from ntsb website search result and export it to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddcc0a3",
   "metadata": {},
   "source": [
    "# Note:\n",
    "* This notebook was used once to extract historical NTSB aviation accident data (2014â€“2024).\n",
    "* The resulting CSV is treated as a static dataset and is included in this repository.\n",
    "* This notebook is provided for transparency and methodology reference only and is not intended to be re-run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de4cfed",
   "metadata": {},
   "source": [
    "### Query Result Webpage for accident data from 01/01/2015 to 31/12/2024\n",
    "https://data.ntsb.gov/carol-main-public/basic-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "lines left 1666\n",
      "Success\n",
      "lines left 1566\n",
      "Success\n",
      "lines left 1466\n",
      "Success\n",
      "lines left 1366\n",
      "Success\n",
      "lines left 1266\n",
      "Success\n",
      "lines left 1166\n",
      "Success\n",
      "lines left 1066\n",
      "Success\n",
      "lines left 966\n",
      "Success\n",
      "lines left 866\n",
      "Success\n",
      "lines left 766\n",
      "Success\n",
      "lines left 666\n",
      "Success\n",
      "lines left 566\n",
      "Success\n",
      "lines left 466\n",
      "Success\n",
      "lines left 366\n",
      "Success\n",
      "lines left 266\n",
      "Success\n",
      "lines left 166\n",
      "Success\n",
      "lines left 66\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def create_100(offset=0):\n",
    "    url = \"https://data.ntsb.gov/carol-main-public/api/Query/Main\" # URL of the NTSB search web page.\n",
    "\n",
    "    # headers extracted from browser\n",
    "    headers = {\n",
    "    'Host': 'data.ntsb.gov',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:143.0) Gecko/20100101 Firefox/143.0',\n",
    "    'Accept': 'application/json',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate, br, zstd',\n",
    "    'Referer': 'https://data.ntsb.gov/carol-main-public/basic-search',\n",
    "    'content-type': 'application/json',\n",
    "    'Origin': 'https://data.ntsb.gov',\n",
    "    'Sec-GPC': '1',\n",
    "    'Connection': 'keep-alive'}\n",
    "\n",
    "    # response extraxted from browser dev tools. Tells the request what data to be asked from website.\n",
    "    payload = {\n",
    "        \"ResultSetSize\": 100, # number of rows we get on one call. Set it to max that is 100.\n",
    "        \"ResultSetOffset\": offset, # starting row of result for each call. After each call we increase it by the number of rows that are displayed to get next rows.\n",
    "        \"QueryGroups\": [\n",
    "            {\n",
    "                \"QueryRules\": [\n",
    "                    {\n",
    "                        \"FieldName\": \"EventDate\",\n",
    "                        \"RuleType\": 0,\n",
    "                        \"Values\": [\"2015-01-01\", \"2024-12-31\"],\n",
    "                        \"Columns\": [\"Event.EventDate\"],\n",
    "                        \"Operator\": \"is in the range\"\n",
    "                    }\n",
    "                ],\n",
    "                \"AndOr\": \"And\"\n",
    "            }\n",
    "        ],\n",
    "        \"AndOr\": \"And\",\n",
    "        \"SortColumn\": \"Event.EventDate\",\n",
    "        \"SortDescending\": True,\n",
    "        \"TargetCollection\": \"cases\",\n",
    "        \"SessionId\": 25525\n",
    "    }\n",
    "\n",
    "\n",
    "    # calling the URL\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(\"Success\")\n",
    "    else:\n",
    "        print(\"Fail\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # After getting an output JSON we start creating df.\n",
    "    ''' \n",
    "    The JSON structure is like this:\n",
    "    json = {\n",
    "    'Result': [bunch of rows containg data for each row.], # From these we scrape the rows dat\n",
    "    'Column': [Contains the name of each 26 columns], # From this we set the columns\n",
    "    'ResultListCount': Total number of rows there are for the query.} #from this we decide how many times we have to call the url to get all data\n",
    "    '''\n",
    "\n",
    "    columns = data['Columns'] # creates header row\n",
    "\n",
    "    # creating data rows\n",
    "    all_rows = np.empty(shape=(len(data['Results']),len(columns)), dtype=object) # we create an numpy matrix to store all the rows data, it's shape is determined by the number of rows and columns.\n",
    "    \n",
    "    for i in range(len(data['Results'])):\n",
    "\n",
    "        rows = data['Results'][i] # every object inside the 'Results' key is a dict and it represents one row\n",
    "\n",
    "        len(rows['Fields']) # there are 26 lines under fields key each corresponds to a column in columns\n",
    "\n",
    "        rows['EntryId'] # this key has a single value. Not relevant for our data. So we'll drop this.\n",
    "\n",
    "        rows = rows['Fields']\n",
    "\n",
    "        # we'll extract only values from each rows\n",
    "        row_values = np.array([]) # An empty array to store values\n",
    "        for row in rows:\n",
    "            value = row['Values'][0] if row['Values'] else None # extract value if exists otherwise set None\n",
    "            row_values = np.append(row_values, value)\n",
    "        all_rows[i,:] = row_values  # filling the matrix with actual row data.\n",
    "    \n",
    "    #from the numpy matrix of rows data we'll create a dataframe and set column values to extracted columns.\n",
    "    df = pd.DataFrame(all_rows, columns=columns)\n",
    "\n",
    "    print(f'lines left {data['ResultListCount'] - offset}') #this tell how many lines are left after each call.\n",
    "\n",
    "    if offset + 100 < data['ResultListCount']: #if there are more lines left after this call\n",
    "\n",
    "        time.sleep(2) # this time gap is set so that the site doesn't get overwhelmed by a lot of repetitive call, and blocks our IP.\n",
    "\n",
    "        df = pd.concat([df, create_100(offset+100)]) # we recall the function by increasing offset by the number of rows we extracted in previous call which is set to 100\n",
    "        \n",
    "    return df # last we return the dataframe\n",
    "\n",
    "\n",
    "\n",
    "df = create_100()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012185ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ntsb_raw_data.csv', index=False) # Saving the collected data in a csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c014723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
